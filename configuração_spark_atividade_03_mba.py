# -*- coding: utf-8 -*-
"""Configuração_spark_atividade_03_MBA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vmbVnLxy4uQ3DDdyzGUh8ovIlo-AHxor
"""

#configurando o ambiente 
!apt-get update -qq
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz
!tar xf spark-3.1.2-bin-hadoop2.7.tgz
!pip install -q findspark

#criando as pastas do Java e Spark
import os
os.environ['JAVA_HOME']="/usr/lib/jvm/java-8-openjdk-amd64"
os.environ['SPARK_HOME']='/content/spark-3.1.2-bin-hadoop2.7'

!pip install -q findspark

#Esse pacote referência o spark
import findspark

findspark.init()

#importa o spark sessão
from pyspark.sql import SparkSession

#Configuração da sessão
spark= SparkSession.builder \
    .master('local[*]') \
    .appName('Aula_spark') \
    .config('spark.iu.port','4050') \
    .getOrCreate()



#!wget -q https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
#!unzip ngrok-stable-linux-amd64.zip

#get_ipython().system_raw('ngrok config add-authtoken 2PieZgGtQEXUx4JnerL86qjMTaI_23TviCWZVtdBFfJ4NGGj')
#get_ipython().system_raw('./ngrok http 4050 &')*/

#!curl -s http://localhost:4050/api/tunnels

#criando uma lista de tupla
data=[('Diego',26),('Josi',23)]

#criando uma lista de colunas
Col=['Nome','Idade']

#Criando um data frame no spark
df=spark.createDataFrame(data,Col)

#Mostrando o data frame 
df.show()

#Monstrando o data frame e modelo de Data frame pandas(py)
df.toPandas()

#criando uma conexão com o Drive caso queira se não desconsidere esse passo 
from google.colab import drive
drive.mount('/content/drive')

#biblioteca de descompactação
import zipfile

#extraindo o arquivo do zip para a mesma pasta 
zipfile.ZipFile('/content/drive/MyDrive/MBA_03/empresas.zip','r').extractall('/content/drive/MyDrive/MBA_03')

#criando um caminho
path='/content/drive/MyDrive/MBA_03/empresas'

#lendo csv 
empresas=spark.read.csv('/content/drive/MyDrive/MBA_03/empresas/part-00000-58983ad4-8444-4405-aec6-9cd3e5413d1b-c000.csv',sep=';',inferSchema=True)

#mostrar as 20 primeiras linhas, dentro do parentes de show podemos escolher quantas linhas vamos
#mostrar igual a função head do python
empresas.show()

#mostrando a quantidade de linhas no Data frame
empresas.count()

#criando uma lista com nome da colunas
Names=['cnpj_basico','razao_social','Natureza_juridica','qualificacao_responsavel','capital_social','porte_empresa','ente_federativo_responsável']

#renomeando as colunas
for index, ColNames in enumerate(Names):
  empresas=empresas.withColumnRenamed(f"_c{index}",ColNames)

#Colunas após a transformação
empresas.columns

#Vendo o tipo de dados dentro do data frame
empresas.printSchema()

#importando double,string e Datatype
from pyspark.sql.types import DoubleType, StringType,DateType

# importando funções matemáticas
from pyspark.sql import functions as f

#Mudando ',' para '.'
empresas=empresas.withColumn('capital_social',f.regexp_replace('capital_social',',','.'))



empresas.show( )

#Transformando string para Double
empresas=empresas.withColumn('capital_social',empresas['capital_social'].cast(DoubleType()))



empresas.printSchema()

empresas \
  .select('*') \
  .show(5,False)

#usando filtro where

empresas \
  .where('capital_social>20 and capital_social<100') \
  .count()

# Usando filtro 'filter'
empresas \
  .filter('capital_social>20 and capital_social<100') \
  .show()

#usando comando SQL no spark
empresas \
  .select('razao_social','porte_empresa') \
  .where('capital_social>20 and capital_social<100') \
  .show(5)

#orderBy
empresas \
  .select('razao_social','porte_empresa') \
  .where('capital_social>20 and capital_social<100') \
  .orderBy('razao_social',ascending =True) \
  .show(5,True)

#sumarizando os dados com a função agg
empresas \
    .select('*')\
    .groupBy('porte_empresa')\
    .agg(
        f.avg('capital_social')
    ) \
    .orderBy('porte_empresa', ascending=True)\
    .show()

#quantas empresas não tem capital social//porte
empresas \
    .select('*')\
    .filter('capital_social==0') \
    .groupBy('porte_empresa')\
    .agg(
        f.count('capital_social')
    ) \
    .orderBy('porte_empresa', ascending=True)\
    .show()

#salvando em formato Parquet 
empresas.write.parquet(
    path='/content/drive/MyDrive/curso-spark/empresas/parquet',
    mode='overwrite'
)

#testando e subindo o arquivo parquet

empresas_2=spark.read.parquet('/content/drive/MyDrive/MBA_03/empresas/parquet/part-00000-d75cf41c-ca89-4849-946a-bcf8eee2a77b-c000.snappy.parquet')

#testando e subindo o arquivo orc
empresas_3=spark.read.parquet('')